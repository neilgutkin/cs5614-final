{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4: PySpark Structured Streaming Using Kafka Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-kafka-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"). \\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==== Q2 ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2.1:** All your code for 2.1 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to 2.1\n",
    "df_streamed_raw = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "  .option(\"subscribe\", \"topic_test1\") \\\n",
    "  .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# convert byte stream to string\n",
    "df_streamed_kv = (df_streamed_raw\n",
    "    .withColumn(\"key\", df_streamed_raw[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df_streamed_raw[\"value\"].cast(StringType())))\n",
    "\n",
    "test_query = (df_streamed_kv \n",
    "              .writeStream \\\n",
    "              .format(\"memory\") # output to memory \\\n",
    "              .outputMode(\"update\") # only write updated rows to the sink \\\n",
    "              .queryName(\"test_query_table\")  # Name of the in memory table \\\n",
    "              .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If all goes well, the following cell should display a table populated with values being streamed from you Kafka producer. NOTE: If you recently ran the producer, it may take a while before the table is populated. Keep rerunning the cell to check for updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from test_query_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells contain code that take the streamed dataframe and formats it properly into a table. If any of the given cells fails, there might be a formatting issue with one of your previous solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"station\", StringType()),\n",
    "    StructField(\"valid\", StringType()),\n",
    "    StructField(\"tmpf\", StringType()),\n",
    "    StructField(\"dwpf\", StringType()),\n",
    "    StructField(\"relh\", StringType()),\n",
    "    StructField(\"feel\", StringType()),\n",
    "    StructField(\"drct\", StringType()),\n",
    "    StructField(\"sped\", StringType()),\n",
    "    StructField(\"alti\", StringType()),\n",
    "    StructField(\"mslp\", StringType()),\n",
    "    StructField(\"p01m\", StringType()),\n",
    "    StructField(\"vsby\", StringType()),\n",
    "    StructField(\"skyc1\", StringType()),\n",
    "    StructField(\"skyl1\", StringType()),\n",
    "    StructField(\"wxcodes\", StringType()),\n",
    "    StructField(\"ice_acceretion_1hr\", StringType()),\n",
    "])\n",
    "\n",
    "# Parse the events from JSON format\n",
    "df_parsed = (df_streamed_kv\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", event_schema))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, unix_timestamp\n",
    "\n",
    "# Here, we need to convert date_time string to date_time object in the \"dd/MMM/yyyy:HH:mm:ss Z\" format.\n",
    "\n",
    "df_formatted = (df_parsed.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\")\n",
    "    ,col(\"value.station\").alias(\"station\")\n",
    "    ,col(\"value.valid\").alias(\"valid\")\n",
    "    ,col(\"value.tmpf\").alias(\"tmpf\")\n",
    "    ,col(\"value.dwpf\").alias(\"dwpf\")\n",
    "    ,col(\"value.relh\").alias(\"relh\")\n",
    "    ,col(\"value.feel\").alias(\"feel\")\n",
    "    ,col(\"value.drct\").alias(\"drct\")\n",
    "    ,col(\"value.sped\").alias(\"sped\")\n",
    "    ,col(\"value.alti\").alias(\"alti\")\n",
    "    ,col(\"value.mslp\").alias(\"mslp\")\n",
    "    ,col(\"value.p01m\").alias(\"p01m\")\n",
    "    ,col(\"value.vsby\").alias(\"vsby\")\n",
    "    ,col(\"value.skyc1\").alias(\"skyc1\")\n",
    "    ,col(\"value.skyl1\").alias(\"skyl1\")\n",
    "    ,col(\"value.wxcodes\").alias(\"wxcodes\")\n",
    "    ,col(\"value.ice_acceretion_1hr\").alias(\"ice_acceretion_1hr\")\n",
    "#     cast(IntegerType()).\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2.2:** All your code for 2.2 should be in the following cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to 2.2\n",
    "query = (df_formatted\n",
    "            .writeStream\n",
    "            .format(\"console\")\n",
    "            .trigger(processingTime='5 seconds')\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"truncate\",'false')\n",
    "            .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name of active streams (This may be useful during debugging)\n",
    "for s in spark.streams.active:\n",
    "    print(f\"ID:{s.id} | NAME:{s.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== Project - Start your feature extraction queries from here ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.1:** All your code for 3.1 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_cumulative_count = (df_formatted\n",
    "            .groupBy(\"event_topic\")\n",
    "            .count()\n",
    "            .orderBy(\"event_topic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_count=(df_cumulative_count\n",
    "                .writeStream\n",
    "                .outputMode(\"complete\")\n",
    "                .format(\"console\")\n",
    "                .trigger(processingTime=\"5 seconds\")\n",
    "                .option(\"truncate\",'false')\n",
    "                .start()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_count.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.2:** All your code for 3.2 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_request_type = (df_formatted\n",
    "                    .groupBy(window(df_formatted.event_timestamp, \"10 seconds\", \"10 seconds\"),df_formatted.request_type)\n",
    "                    .count()\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_request_type= (df_request_type\n",
    "          .writeStream\n",
    "          .outputMode(\"complete\")\n",
    "          .format(\"console\")\n",
    "          .option(\"truncate\",'false')\n",
    "          .trigger(processingTime=\"5 seconds\")\n",
    "          .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_request_type.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.3:** All your code for 3.3 should be in the following cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "smAvg = (df_formatted\n",
    "               .groupBy(window(df_formatted.event_timestamp, \"10 seconds\", \"10 seconds\"))\n",
    "               .agg(avg(\"response_size\")\n",
    "               .alias(\"moving_average\"))\n",
    "               .writeStream\n",
    "               .outputMode(\"complete\")\n",
    "               .format(\"console\")\n",
    "               .option(\"truncate\",'false')\n",
    "               .trigger(processingTime=\"10 seconds\")\n",
    "               .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smAvg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.4:** All your code for 3.4 should be in the following cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_select = (df_formatted\n",
    "               .groupBy(window(\"event_timestamp\", \"10 seconds\").alias('Time_Window'))\n",
    "               .agg(\n",
    "                    round(avg(\"response_size\"),4).alias(\"Avg\"),\n",
    "                    round(stddev_samp(\"response_size\"),4).alias(\"Standard_Dev\"),\n",
    "                    count(\"*\").alias(\"Count\"),\n",
    "                    collect_list(\"response_size\").alias(\"List\")\n",
    "                   )\n",
    "               .select(\"Time_Window\", \"Avg\", \"Standard_Dev\", \"Count\", \"List\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_final= (df_select\n",
    "                    .writeStream\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"console\")\n",
    "                    .trigger(processingTime=\"10 seconds\")\n",
    "                    .option(\"truncate\",'false')\n",
    "                    .start()\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_final.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_explode_query = df_select.select(\"Time_Window\", \"Avg\", \"Standard_Dev\", explode(\"List\").alias(\"ResponseSize\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explode_final= (df_explode_query\n",
    "                    .writeStream\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"console\")\n",
    "                    .trigger(processingTime=\"10 seconds\")\n",
    "                    .option(\"truncate\",'false')\n",
    "                    .start()\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explode_final.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = (df_explode_query.withColumn('z_score', expr('(ResponseSize - Avg) / Standard_Dev'))\n",
    "            .filter(\"z_score > 1 or z_score < -1\")\n",
    "            .filter(~isnan(col(\"z_score\")))\n",
    "            .select(\"Time_Window\", \"Avg\", \"Standard_Dev\",\"ResponseSize\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data= (df_score\n",
    "            .writeStream\n",
    "            .outputMode(\"complete\")\n",
    "            .format(\"console\")\n",
    "            .option(\"truncate\",'false')\n",
    "            .trigger(processingTime=\"10 seconds\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
