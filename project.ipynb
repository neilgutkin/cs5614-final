{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Apache Spark Scala API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shows how to connect Jupyter notebooks to a Spark cluster to process data using Spark Scala API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Get Spark\n",
    "\n",
    "Let's start by importing Apache Spark from Maven repository (mind the Spark **version**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                 ;\u001b[39m"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be disabling Spark internal logs to let us focus on its API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger};\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger};\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Connection\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** application name displayed at the [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** Spark Master URL, same used by Spark Workers;\n",
    "+ **spark.executor.memory:** must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@323778fc"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.\n",
    "            builder().\n",
    "            appName(\"scala-spark-notebook\").\n",
    "            master(\"spark://spark-master:7077\").\n",
    "            config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\").\n",
    "            config(\"spark.executor.memory\", \"512m\").\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More confs for SparkSession object in standalone mode can be added using the **config** method. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SparkSession.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Introduction\n",
    "\n",
    "We will be using Spark Scala API to read, process and write data. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Read\n",
    "\n",
    "Let's read some UK's macroeconomic data ([source](https://www.kaggle.com/bank-of-england/a-millennium-of-macroeconomic-data)) from the cluster's simulated **Hadoop distributed file system (HDFS)** into a Spark dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then display some dataframe metadata, such as the number of rows and cols and its schema (cols name and type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mdf_streamed_raw\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val df_streamed_raw = (spark\n",
    "        .readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "        .option(\"subscribe\", \"topic_test\")\n",
    "        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, from_json}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdf_streamed_kv\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: string, value: string ... 5 more fields]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions.{col, from_json}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val df_streamed_kv = df_streamed_raw\n",
    "  .withColumn(\"key\", col(\"key\").cast(\"STRING\"))\n",
    "  .withColumn(\"value\", col(\"value\").cast(\"STRING\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtest1\u001b[39m: \u001b[32mstreaming\u001b[39m.\u001b[32mStreamingQuery\u001b[39m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2539b9a0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test1=(df_streamed_kv \n",
    "              .writeStream \n",
    "              .format(\"console\") \n",
    "              .outputMode(\"update\")\n",
    "              .queryName(\"test_query_table\")\n",
    "              .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from test_query_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36meventSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"valid\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"tmpf\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"dwpf\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"relh\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"feel\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"drct\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"sped\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"alti\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"p01m\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"vsby\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"skyc1\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"skyl1\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"wxcodes\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"station_encoded\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"skyc1_encoded\"\u001b[39m, StringType, true, {})\n",
       ")\n",
       "\u001b[36mpersonDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [valid: string, tmpf: string ... 13 more fields]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eventSchema = new StructType()\n",
    "  .add(\"valid\", StringType)\n",
    "  .add(\"tmpf\", StringType)\n",
    "  .add(\"dwpf\", StringType)\n",
    "  .add(\"relh\", StringType)\n",
    "  .add(\"feel\", StringType)\n",
    "  .add(\"drct\", StringType)\n",
    "  .add(\"sped\", StringType)\n",
    "  .add(\"alti\", StringType)\n",
    "  .add(\"p01m\", StringType)\n",
    "  .add(\"vsby\", StringType)\n",
    "  .add(\"skyc1\", StringType)\n",
    "  .add(\"skyl1\", StringType)\n",
    "  .add(\"wxcodes\", StringType)\n",
    "  .add(\"station_encoded\", StringType)\n",
    "  .add(\"skyc1_encoded\", StringType)\n",
    "\n",
    "\n",
    " val personDF = df_streamed_kv.select(from_json(col(\"value\"), eventSchema).as(\"data\"))\n",
    "   .select(\"data.*\")\n",
    "// val dfParsed = df_streamed_kv\n",
    "//   .withColumn(\"value\", from_json(col(\"value\"), eventSchema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+----+----+----+----+----+----+----+----+----+-----+-----+-------+---------------+-------------+\n",
      "|valid|tmpf|dwpf|relh|feel|drct|sped|alti|p01m|vsby|skyc1|skyl1|wxcodes|station_encoded|skyc1_encoded|\n",
      "+-----+----+----+----+----+----+----+----+----+----+-----+-----+-------+---------------+-------------+\n",
      "+-----+----+----+----+----+----+----+----+----+----+-----+-----+-------+---------------+-------------+\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.streaming.StreamingQueryException: Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = 801b9d71-5b79-4973-99ea-3f9d0a37e9f4, runId = 3df80dac-f327-4da9-9962-dbb1c248bdf3]\nCurrent Committed Offsets: {KafkaV2[Subscribe[topic_test]]: {\"topic_test\":{\"0\":13923}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[topic_test]]: {\"topic_test\":{\"0\":13924}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- Project [data#3488.valid AS valid#3490, data#3488.tmpf AS tmpf#3491, data#3488.dwpf AS dwpf#3492, data#3488.relh AS relh#3493, data#3488.feel AS feel#3494, data#3488.drct AS drct#3495, data#3488.sped AS sped#3496, data#3488.alti AS alti#3497, data#3488.p01m AS p01m#3498, data#3488.vsby AS vsby#3499, data#3488.skyc1 AS skyc1#3500, data#3488.skyl1 AS skyl1#3501, data#3488.wxcodes AS wxcodes#3502, data#3488.station_encoded AS station_encoded#3503, data#3488.skyc1_encoded AS skyc1_encoded#3504]\n   +- Project [from_json(StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(station_encoded,StringType,true), StructField(skyc1_encoded,StringType,true), value#3067, Some(Etc/UTC)) AS data#3488]\n      +- Project [key#3059, cast(value#3046 as string) AS value#3067, topic#3047, partition#3048, offset#3049L, timestamp#3050, timestampType#3051]\n         +- Project [cast(key#3045 as string) AS key#3059, value#3046, topic#3047, partition#3048, offset#3049L, timestamp#3050, timestampType#3051]\n            +- StreamingDataSourceV2Relation [key#3045, value#3046, topic#3047, partition#3048, offset#3049L, timestamp#3050, timestampType#3051], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@2694076c, KafkaV2[Subscribe[topic_test]]\n\u001b[39m\n  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m355\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m245\u001b[39m)\n\u001b[31morg.apache.spark.SparkException: Writing job aborted.\u001b[39m\n  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m413\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m361\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m322\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m329\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m45\u001b[39m)\n  org.apache.spark.sql.Dataset.collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3625\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$collect$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2938\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$withAction$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3616\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m87\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m763\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3614\u001b[39m)\n  org.apache.spark.sql.Dataset.collect(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2938\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m576\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m87\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m763\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m571\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m352\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m571\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m223\u001b[39m)\n  scala.runtime.java8.JFunction0$mcV$sp.apply(\u001b[32mJFunction0$mcV$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m352\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m191\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(\u001b[32mTriggerExecutor.scala\u001b[39m:\u001b[32m57\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m185\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m334\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m245\u001b[39m)\n\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 4 times, most recent failure: Lost task 0.3 in stage 57.0 (TID 150, 172.18.0.6, executor 0): java.lang.ClassNotFoundException: org.apache.spark.sql.kafka010.KafkaBatchInputPartition\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:405)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2023\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1972\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1971\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1971\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m950\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m950\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m950\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2203\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2152\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2141\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m752\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2093\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m382\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m361\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m322\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m329\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m45\u001b[39m)\n  org.apache.spark.sql.Dataset.collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3625\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$collect$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2938\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$withAction$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3616\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m87\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m763\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3614\u001b[39m)\n  org.apache.spark.sql.Dataset.collect(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2938\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m576\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m87\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m763\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m571\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m352\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m571\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m223\u001b[39m)\n  scala.runtime.java8.JFunction0$mcV$sp.apply(\u001b[32mJFunction0$mcV$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m352\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m191\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(\u001b[32mTriggerExecutor.scala\u001b[39m:\u001b[32m57\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m185\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m334\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m245\u001b[39m)\n\u001b[31mjava.lang.ClassNotFoundException: org.apache.spark.sql.kafka010.KafkaBatchInputPartition\u001b[39m\n  java.net.URLClassLoader.findClass(\u001b[32mURLClassLoader.java\u001b[39m:\u001b[32m387\u001b[39m)\n  java.lang.ClassLoader.loadClass(\u001b[32mClassLoader.java\u001b[39m:\u001b[32m418\u001b[39m)\n  java.lang.ClassLoader.loadClass(\u001b[32mClassLoader.java\u001b[39m:\u001b[32m351\u001b[39m)\n  java.lang.Class.forName0(\u001b[32mNative Method\u001b[39m)\n  java.lang.Class.forName(\u001b[32mClass.java\u001b[39m:\u001b[32m348\u001b[39m)\n  org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m68\u001b[39m)\n  java.io.ObjectInputStream.readNonProxyDesc(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1988\u001b[39m)\n  java.io.ObjectInputStream.readClassDesc(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1852\u001b[39m)\n  java.io.ObjectInputStream.readOrdinaryObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2186\u001b[39m)\n  java.io.ObjectInputStream.readObject0(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1669\u001b[39m)\n  java.io.ObjectInputStream.defaultReadFields(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2431\u001b[39m)\n  java.io.ObjectInputStream.readSerialData(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2355\u001b[39m)\n  java.io.ObjectInputStream.readOrdinaryObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2213\u001b[39m)\n  java.io.ObjectInputStream.readObject0(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1669\u001b[39m)\n  java.io.ObjectInputStream.defaultReadFields(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2431\u001b[39m)\n  java.io.ObjectInputStream.readSerialData(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2355\u001b[39m)\n  java.io.ObjectInputStream.readOrdinaryObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2213\u001b[39m)\n  java.io.ObjectInputStream.readObject0(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1669\u001b[39m)\n  java.io.ObjectInputStream.readObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m503\u001b[39m)\n  java.io.ObjectInputStream.readObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m461\u001b[39m)\n  org.apache.spark.serializer.JavaDeserializationStream.readObject(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m76\u001b[39m)\n  org.apache.spark.serializer.JavaSerializerInstance.deserialize(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m115\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m750\u001b[39m)"
     ]
    }
   ],
   "source": [
    "personDF.writeStream\n",
    "      .format(\"console\")\n",
    "      .outputMode(\"append\")\n",
    "      .start()\n",
    "      .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, to_timestamp, unix_timestamp}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.IntegerType\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdfFormatted\u001b[39m: \u001b[32mDataFrame\u001b[39m = [event_key: string, event_topic: string ... 16 more fields]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, to_timestamp, unix_timestamp}\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "\n",
    "val dfFormatted = dfParsed.select(\n",
    "    col(\"key\").alias(\"event_key\"),\n",
    "    col(\"topic\").alias(\"event_topic\"),\n",
    "    col(\"timestamp\").alias(\"event_timestamp\"),\n",
    "    col(\"value.valid\").alias(\"valid\"),\n",
    "    col(\"value.tmpf\").alias(\"tmpf\"),\n",
    "    col(\"value.dwpf\").alias(\"dwpf\"),\n",
    "    col(\"value.relh\").alias(\"relh\"),\n",
    "    col(\"value.feel\").alias(\"feel\"),\n",
    "    col(\"value.drct\").alias(\"drct\"),\n",
    "    col(\"value.sped\").alias(\"sped\"),\n",
    "    col(\"value.alti\").alias(\"alti\"),\n",
    "    col(\"value.p01m\").alias(\"p01m\"),\n",
    "    col(\"value.vsby\").alias(\"vsby\"),\n",
    "    col(\"value.skyc1\").alias(\"skyc1\"),\n",
    "    col(\"value.skyl1\").alias(\"skyl1\"),\n",
    "    col(\"value.wxcodes\").alias(\"wxcodes\"),\n",
    "    col(\"value.station_encoded\").alias(\"station_encoded\"),\n",
    "    col(\"value.skyc1_encoded\").alias(\"skyc1_encoded\"),\n",
    "\n",
    ")\n",
    "// .select(\n",
    "//     col(\"event_key\"),\n",
    "//     col(\"event_topic\"),\n",
    "//     col(\"event_timestamp\"),\n",
    "//     col(\"valid\"),\n",
    "//     col(\"tmpf\"),\n",
    "//     col(\"dwpf\"),\n",
    "//     col(\"relh\"),\n",
    "//     col(\"feel\"),\n",
    "//     col(\"drct\"),\n",
    "//     col(\"sped\"),\n",
    "//     col(\"alti\"),\n",
    "//     col(\"p01m\"),\n",
    "//     col(\"vsby\"),\n",
    "//     col(\"skyc1\"),\n",
    "//     col(\"skyl1\"),\n",
    "//     col(\"wxcodes\"),\n",
    "//     col(\"station_encoded\"),\n",
    "//     col(\"skyc1_encoded\"),\n",
    "// )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----------+---------------+-----+----+----+----+----+----+----+----+----+----+-----+-----+-------+---------------+-------------+\n",
      "|event_key|event_topic|event_timestamp|valid|tmpf|dwpf|relh|feel|drct|sped|alti|p01m|vsby|skyc1|skyl1|wxcodes|station_encoded|skyc1_encoded|\n",
      "+---------+-----------+---------------+-----+----+----+----+----+----+----+----+----+----+-----+-----+-------+---------------+-------------+\n",
      "+---------+-----------+---------------+-----+----+----+----+----+----+----+----+----+----+-----+-----+-------+---------------+-------------+\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.streaming.StreamingQueryException: Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = 4ef5d135-9a60-4067-8e36-70eb181a5ac7, runId = dc47ac95-e5e1-43b5-b841-2c2538f8a62c]\nCurrent Committed Offsets: {KafkaV2[Subscribe[topic_test]]: {\"topic_test\":{\"0\":14046}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[topic_test]]: {\"topic_test\":{\"0\":14047}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- Project [key#1425 AS event_key#3826, topic#1413 AS event_topic#3827, timestamp#1416 AS event_timestamp#3828, value#1441.valid AS valid#3829, value#1441.tmpf AS tmpf#3830, value#1441.dwpf AS dwpf#3831, value#1441.relh AS relh#3832, value#1441.feel AS feel#3833, value#1441.drct AS drct#3834, value#1441.sped AS sped#3835, value#1441.alti AS alti#3836, value#1441.p01m AS p01m#3837, value#1441.vsby AS vsby#3838, value#1441.skyc1 AS skyc1#3839, value#1441.skyl1 AS skyl1#3840, value#1441.wxcodes AS wxcodes#3841, value#1441.station_encoded AS station_encoded#3842, value#1441.skyc1_encoded AS skyc1_encoded#3843]\n   +- Project [key#1425, from_json(StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(station_encoded,StringType,true), StructField(skyc1_encoded,StringType,true), value#1433, Some(Etc/UTC)) AS value#1441, topic#1413, partition#1414, offset#1415L, timestamp#1416, timestampType#1417]\n      +- Project [key#1425, cast(value#1412 as string) AS value#1433, topic#1413, partition#1414, offset#1415L, timestamp#1416, timestampType#1417]\n         +- Project [cast(key#1411 as string) AS key#1425, value#1412, topic#1413, partition#1414, offset#1415L, timestamp#1416, timestampType#1417]\n            +- StreamingDataSourceV2Relation [key#1411, value#1412, topic#1413, partition#1414, offset#1415L, timestamp#1416, timestampType#1417], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@2a3e186, KafkaV2[Subscribe[topic_test]]\n\u001b[39m\n  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m355\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m245\u001b[39m)\n\u001b[31morg.apache.spark.SparkException: Writing job aborted.\u001b[39m\n  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m413\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m361\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m322\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m329\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m45\u001b[39m)\n  org.apache.spark.sql.Dataset.collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3625\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$collect$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2938\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$withAction$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3616\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m87\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m763\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3614\u001b[39m)\n  org.apache.spark.sql.Dataset.collect(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2938\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m576\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m87\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m763\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m571\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m352\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m571\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m223\u001b[39m)\n  scala.runtime.java8.JFunction0$mcV$sp.apply(\u001b[32mJFunction0$mcV$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m352\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m191\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(\u001b[32mTriggerExecutor.scala\u001b[39m:\u001b[32m57\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m185\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m334\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m245\u001b[39m)\n\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 61.0 failed 4 times, most recent failure: Lost task 0.3 in stage 61.0 (TID 160, 172.18.0.7, executor 1): java.lang.ClassNotFoundException: org.apache.spark.sql.kafka010.KafkaBatchInputPartition\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:405)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2023\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1972\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1971\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1971\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m950\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m950\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m950\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2203\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2152\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2141\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m752\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2093\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m382\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m361\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m322\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(\u001b[32mWriteToDataSourceV2Exec.scala\u001b[39m:\u001b[32m329\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(\u001b[32mV2CommandExec.scala\u001b[39m:\u001b[32m45\u001b[39m)\n  org.apache.spark.sql.Dataset.collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3625\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$collect$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2938\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$withAction$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3616\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m87\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m763\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3614\u001b[39m)\n  org.apache.spark.sql.Dataset.collect(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2938\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m576\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m87\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m763\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m571\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m352\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m571\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m223\u001b[39m)\n  scala.runtime.java8.JFunction0$mcV$sp.apply(\u001b[32mJFunction0$mcV$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m352\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m191\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(\u001b[32mTriggerExecutor.scala\u001b[39m:\u001b[32m57\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m185\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m334\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m245\u001b[39m)\n\u001b[31mjava.lang.ClassNotFoundException: org.apache.spark.sql.kafka010.KafkaBatchInputPartition\u001b[39m\n  java.net.URLClassLoader.findClass(\u001b[32mURLClassLoader.java\u001b[39m:\u001b[32m387\u001b[39m)\n  java.lang.ClassLoader.loadClass(\u001b[32mClassLoader.java\u001b[39m:\u001b[32m418\u001b[39m)\n  java.lang.ClassLoader.loadClass(\u001b[32mClassLoader.java\u001b[39m:\u001b[32m351\u001b[39m)\n  java.lang.Class.forName0(\u001b[32mNative Method\u001b[39m)\n  java.lang.Class.forName(\u001b[32mClass.java\u001b[39m:\u001b[32m348\u001b[39m)\n  org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m68\u001b[39m)\n  java.io.ObjectInputStream.readNonProxyDesc(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1988\u001b[39m)\n  java.io.ObjectInputStream.readClassDesc(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1852\u001b[39m)\n  java.io.ObjectInputStream.readOrdinaryObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2186\u001b[39m)\n  java.io.ObjectInputStream.readObject0(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1669\u001b[39m)\n  java.io.ObjectInputStream.defaultReadFields(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2431\u001b[39m)\n  java.io.ObjectInputStream.readSerialData(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2355\u001b[39m)\n  java.io.ObjectInputStream.readOrdinaryObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2213\u001b[39m)\n  java.io.ObjectInputStream.readObject0(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1669\u001b[39m)\n  java.io.ObjectInputStream.defaultReadFields(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2431\u001b[39m)\n  java.io.ObjectInputStream.readSerialData(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2355\u001b[39m)\n  java.io.ObjectInputStream.readOrdinaryObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m2213\u001b[39m)\n  java.io.ObjectInputStream.readObject0(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m1669\u001b[39m)\n  java.io.ObjectInputStream.readObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m503\u001b[39m)\n  java.io.ObjectInputStream.readObject(\u001b[32mObjectInputStream.java\u001b[39m:\u001b[32m461\u001b[39m)\n  org.apache.spark.serializer.JavaDeserializationStream.readObject(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m76\u001b[39m)\n  org.apache.spark.serializer.JavaSerializerInstance.deserialize(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m115\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m750\u001b[39m)"
     ]
    }
   ],
   "source": [
    "dfFormatted.writeStream\n",
    "      .format(\"console\")\n",
    "      .outputMode(\"append\")\n",
    "      .start()\n",
    "      .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.streaming.Trigger\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mquery\u001b[39m: \u001b[32mstreaming\u001b[39m.\u001b[32mStreamingQuery\u001b[39m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@acdd836"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "val query = dfFormatted.writeStream\n",
    "  .format(\"console\")\n",
    "  .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n",
    "  .outputMode(\"append\")\n",
    "  .option(\"truncate\", false)\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres4\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m841L\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m77\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will get UK's population and unemployment rate thoughtout the years. Let's start by selecting the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">unemployment</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = <style>@keyframes fadein { from { opacity: 0; } to { opacity: 1; } }</style><span style=\"animation: fadein 2s;\">[year: string, population: string ... 1 more field]</span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36munemployment\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, population: string ... 1 more field]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var unemployment = data.select(\"Description\", \"Population (GB+NI)\", \"Unemployment rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------+\n",
      "|Description|Population (GB+NI)|Unemployment rate|\n",
      "+-----------+------------------+-----------------+\n",
      "|      Units|              000s|                %|\n",
      "|       1209|              null|             null|\n",
      "|       1210|              null|             null|\n",
      "|       1211|              null|             null|\n",
      "|       1212|              null|             null|\n",
      "|       1213|              null|             null|\n",
      "|       1214|              null|             null|\n",
      "|       1215|              null|             null|\n",
      "|       1216|              null|             null|\n",
      "|       1217|              null|             null|\n",
      "+-----------+------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unemployment.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully selected the desired columns but two problems were found:\n",
    "+ The first line contains no data but the unit of measurement of each column;\n",
    "+ There are many years with missing population and unemployment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then remove the first line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcols_description\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Description: string, Population (GB+NI): string ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cols_description = unemployment.filter(unemployment(\"Description\") === \"Units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------+\n",
      "|Description|Population (GB+NI)|Unemployment rate|\n",
      "+-----------+------------------+-----------------+\n",
      "|      Units|              000s|                %|\n",
      "+-----------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_description.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.join(cols_description, unemployment(\"Description\") === cols_description(\"Description\"), \"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------+\n",
      "|Description|Population (GB+NI)|Unemployment rate|\n",
      "+-----------+------------------+-----------------+\n",
      "|       1209|              null|             null|\n",
      "|       1210|              null|             null|\n",
      "|       1211|              null|             null|\n",
      "|       1212|              null|             null|\n",
      "|       1213|              null|             null|\n",
      "|       1214|              null|             null|\n",
      "|       1215|              null|             null|\n",
      "|       1216|              null|             null|\n",
      "|       1217|              null|             null|\n",
      "|       1218|              null|             null|\n",
      "+-----------+------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unemployment.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now, let's drop the dataframe rows with missing data and refactor its columns names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.\n",
    "                withColumnRenamed(\"Description\", \"year\").\n",
    "                withColumnRenamed(\"Population (GB+NI)\", \"population\").\n",
    "                withColumnRenamed(\"Unemployment rate\", \"unemployment_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+\n",
      "|year|population|unemployment_rate|\n",
      "+----+----------+-----------------+\n",
      "|1855|     23241|             3.73|\n",
      "|1856|     23466|             3.52|\n",
      "|1857|     23689|             3.95|\n",
      "|1858|     23914|             5.23|\n",
      "|1859|     24138|             3.27|\n",
      "|1860|     24360|             2.94|\n",
      "|1861|     24585|             3.72|\n",
      "|1862|     24862|             4.68|\n",
      "|1863|     25142|             4.15|\n",
      "|1864|     25425|             2.99|\n",
      "+----+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unemployment.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we persist the unemployment data into the cluster's simulated **HDFS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment.repartition(1).write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \",\").option(\"header\", \"true\").save(\"data/uk-macroeconomic-unemployment-data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
