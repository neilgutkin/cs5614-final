{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4: PySpark Structured Streaming Using Kafka Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-827a1b59-0058-4541-8bbe-1726ca509d9f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1084ms :: artifacts dl 51ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-827a1b59-0058-4541-8bbe-1726ca509d9f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/19ms)\n",
      "23/04/12 06:38:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-kafka-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"). \\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==== Q2 ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2.1:** All your code for 2.1 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to 2.1\n",
    "df_streamed_raw = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "  .option(\"subscribe\", \"topic_test1\") \\\n",
    "  .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:38:51 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-47351cc7-3d4d-4584-9f83-d15057113f75. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# convert byte stream to string\n",
    "df_streamed_kv = (df_streamed_raw\n",
    "    .withColumn(\"key\", df_streamed_raw[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df_streamed_raw[\"value\"].cast(StringType())))\n",
    "\n",
    "test_query = (df_streamed_kv \n",
    "              .writeStream \\\n",
    "              .format(\"memory\") # output to memory \\\n",
    "              .outputMode(\"update\") # only write updated rows to the sink \\\n",
    "              .queryName(\"test_query_table\")  # Name of the in memory table \\\n",
    "              .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If all goes well, the following cell should display a table populated with values being streamed from you Kafka producer. NOTE: If you recently ran the producer, it may take a while before the table is populated. Keep rerunning the cell to check for updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from test_query_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:38:57 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3616af31 is aborting.\n",
      "23/04/12 06:38:57 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3616af31 aborted.\n"
     ]
    }
   ],
   "source": [
    "test_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells contain code that take the streamed dataframe and formats it properly into a table. If any of the given cells fails, there might be a formatting issue with one of your previous solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"station\", StringType()),\n",
    "    StructField(\"valid\", StringType()),\n",
    "    StructField(\"tmpf\", StringType()),\n",
    "    StructField(\"dwpf\", StringType()),\n",
    "    StructField(\"relh\", StringType()),\n",
    "    StructField(\"feel\", StringType()),\n",
    "    StructField(\"drct\", StringType()),\n",
    "    StructField(\"sped\", StringType()),\n",
    "    StructField(\"alti\", StringType()),\n",
    "    StructField(\"mslp\", StringType()),\n",
    "    StructField(\"p01m\", StringType()),\n",
    "    StructField(\"vsby\", StringType()),\n",
    "    StructField(\"skyc1\", StringType()),\n",
    "    StructField(\"skyl1\", StringType()),\n",
    "    StructField(\"wxcodes\", StringType()),\n",
    "    StructField(\"ice_acceretion_1hr\", StringType()),\n",
    "])\n",
    "\n",
    "# Parse the events from JSON format\n",
    "df_parsed = (df_streamed_kv\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", event_schema))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, unix_timestamp\n",
    "\n",
    "# Here, we need to convert date_time string to date_time object in the \"dd/MMM/yyyy:HH:mm:ss Z\" format.\n",
    "\n",
    "df_formatted = (df_parsed.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\")\n",
    "    ,col(\"value.station\").alias(\"station\")\n",
    "#     ,col(\"value.valid\").alias(\"valid\")\n",
    "    ,to_timestamp(col(\"value.valid\"), \"yyyy-MM-dd HH:mm\").alias(\"valid\")\n",
    "    ,col(\"value.tmpf\").alias(\"tmpf\")\n",
    "    ,col(\"value.dwpf\").alias(\"dwpf\")\n",
    "    ,col(\"value.relh\").alias(\"relh\")\n",
    "    ,col(\"value.feel\").alias(\"feel\")\n",
    "    ,col(\"value.drct\").alias(\"drct\")\n",
    "    ,col(\"value.sped\").alias(\"sped\")\n",
    "    ,col(\"value.alti\").alias(\"alti\")\n",
    "    ,col(\"value.mslp\").alias(\"mslp\")\n",
    "    ,col(\"value.p01m\").alias(\"p01m\")\n",
    "    ,col(\"value.vsby\").alias(\"vsby\")\n",
    "    ,col(\"value.skyc1\").alias(\"skyc1\")\n",
    "    ,col(\"value.skyl1\").alias(\"skyl1\")\n",
    "    ,col(\"value.wxcodes\").alias(\"wxcodes\")\n",
    "    ,col(\"value.ice_acceretion_1hr\").alias(\"ice_acceretion_1hr\")\n",
    "#     cast(IntegerType()).\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2.2:** All your code for 2.2 should be in the following cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:38:58 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-db146522-2460-4cb2-8a4e-8ccabfd3c47e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "# Answer to 2.2\n",
    "query = (df_formatted\n",
    "            .writeStream\n",
    "            .format(\"console\")\n",
    "            .trigger(processingTime='5 seconds')\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"truncate\",'false')\n",
    "            .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:6d9fda1d-108c-4c1d-9e75-15271de0dc27 | NAME:None\n"
     ]
    }
   ],
   "source": [
    "# Print the name of active streams (This may be useful during debugging)\n",
    "for s in spark.streams.active:\n",
    "    print(f\"ID:{s.id} | NAME:{s.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== Project - Start your feature extraction queries from here ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.1:** All your code for 3.1 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:38:59 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 172.18.0.7, executor 1): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_day = df_formatted.filter((hour(df_formatted.valid) >= 8) & (hour(df_formatted.valid) < 19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_add, to_date\n",
    "# Subtract 7 days from the current date to get the date 1 week ago\n",
    "one_week_ago = date_add(to_date(col(\"valid\"), \"yyyy-MM-dd HH:mm\"), -7)\n",
    "# Filter out the rows where the valid timestamp is one week old or older\n",
    "filtered_df = df_day.where(col(\"valid\").cast(\"date\") >= one_week_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:38:59 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-75b6131c-30f9-4569-9e53-1b80045713db. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "query1 = (filtered_df\n",
    "            .writeStream\n",
    "            .format(\"console\")\n",
    "            .trigger(processingTime='5 seconds')\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"truncate\",'false')\n",
    "            .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:39:00 WARN Shell: Interrupted while joining on: Thread[Thread-67,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:629)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:580)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:482)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:491)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:532)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:509)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1066)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:183)\n",
      "\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:201)\n",
      "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:730)\n",
      "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:726)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:733)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.mkdirs(CheckpointFileManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.<init>(HDFSMetadataLog.scala:66)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaSourceInitialOffsetWriter.<init>(KafkaSourceInitialOffsetWriter.scala:32)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:147)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$5(MicroBatchExecution.scala:378)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:378)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----------+---------------+-------+-----+----+----+----+----+----+----+----+----+----+----+-----+-----+-------+------------------+\n",
      "|event_key|event_topic|event_timestamp|station|valid|tmpf|dwpf|relh|feel|drct|sped|alti|mslp|p01m|vsby|skyc1|skyl1|wxcodes|ice_acceretion_1hr|\n",
      "+---------+-----------+---------------+-------+-----+----+----+----+----+----+----+----+----+----+----+-----+-----+-------+------------------+\n",
      "+---------+-----------+---------------+-------+-----+----+----+----+----+----+----+----+----+----+----+-----+-----+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_night=df_formatted.filter(\n",
    "    ((hour(df_formatted.valid) >= 19) & (hour(df_formatted.valid) <=23)) |\n",
    "    ((hour(df_formatted.valid) >= 0) & (hour(df_formatted.valid) < 8))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_add, to_date\n",
    "# Subtract 7 days from the current date to get the date 1 week ago\n",
    "one_week_ago = date_add(to_date(col(\"valid\"), \"yyyy-MM-dd HH:mm\"), -7)\n",
    "# Filter out the rows where the valid timestamp is one week old or older\n",
    "filtered_df1 = df_night.where(col(\"valid\").cast(\"date\") >= one_week_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:39:03 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fcf1b5ef-aec9-47d9-94f6-99182339d412. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "query2 = (filtered_df1\n",
    "            .writeStream\n",
    "            .format(\"console\")\n",
    "            .trigger(processingTime='5 seconds')\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"truncate\",'false')\n",
    "            .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, window,stddev\n",
    "# assuming your input stream is named `input_stream`\n",
    "df_stream_day = (df_day\n",
    "   .withWatermark(\"valid\", \"7 days\")\\\n",
    "    .groupBy(\"station\", window(\"valid\", \"7 days\"))\\\n",
    "    .agg(avg(\"tmpf\").alias(\"avg_temp\"), stddev(\"tmpf\").alias(\"stddev_temp\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:39:04 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ef9d9c6c-0f15-4431-aef9-f0e65cde9994. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "query_3 = df_stream_day\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, window,stddev\n",
    "# assuming your input stream is named `input_stream`\n",
    "df_stream_night = (df_night\n",
    "   .withWatermark(\"valid\", \"15 minutes\")\\\n",
    "    .groupBy(\"station\", window(\"valid\", \"7 days\"))\\\n",
    "    .agg(avg(\"tmpf\").alias(\"avg_temp\"), stddev(\"tmpf\").alias(\"stddev_temp\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:41:39 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a657be28-0150-478b-816a-38b23d2a396b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+------+--------+-----------+\n",
      "|station|window|avg_temp|stddev_temp|\n",
      "+-------+------+--------+-----------+\n",
      "+-------+------+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:41:55 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 16266 milliseconds\n",
      "23/04/12 06:42:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12368 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+------+--------+-----------+\n",
      "|station|window|avg_temp|stddev_temp|\n",
      "+-------+------+--------+-----------+\n",
      "+-------+------+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:==================>                                    (69 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "query_4 = df_stream_night\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:42:13 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@329f5e51 is aborting.\n",
      "23/04/12 06:42:13 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@329f5e51 aborted.\n",
      "23/04/12 06:42:13 WARN TaskSetManager: Lost task 78.0 in stage 15.0 (TID 1086, 172.18.0.7, executor 1): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "query_4.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_night=df_stream_day.union(df_stream_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:42:20 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-627c5742-0fda-4538-a649-7c99e4743ada. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;;\nUnion\n:- Aggregate [station#113, window#377-T604800000ms], [station#113, window#377-T604800000ms AS window#337-T604800000ms, avg(cast(tmpf#115 as double)) AS avg_temp#358, stddev_samp(cast(tmpf#115 as double)) AS stddev_temp#368]\n:  +- Filter isnotnull(valid#114-T604800000ms)\n:     +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) as double) = (cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 604800000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) as double) = (cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 604800000000) + 0) + 604800000000), LongType, TimestampType)) AS window#377-T604800000ms, event_key#110, event_topic#111, event_timestamp#112, station#113, valid#114-T604800000ms, tmpf#115, dwpf#116, relh#117, feel#118, drct#119, sped#120, alti#121, mslp#122, p01m#123, vsby#124, skyc1#125, skyl1#126, wxcodes#127, ice_acceretion_1hr#128]\n:        +- EventTimeWatermark valid#114: timestamp, 7 days\n:           +- Filter ((hour(valid#114, Some(Etc/UTC)) >= 8) AND (hour(valid#114, Some(Etc/UTC)) < 19))\n:              +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n:                 +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n:                    +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n:                       +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n:                          +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n+- Project [station#113 AS station#1033, window#402-T900000ms AS window#1034-T900000ms, avg_temp#423 AS avg_temp#1035, stddev_temp#433 AS stddev_temp#1036]\n   +- Aggregate [station#113, window#442-T900000ms], [station#113, window#442-T900000ms AS window#402-T900000ms, avg(cast(tmpf#115 as double)) AS avg_temp#423, stddev_samp(cast(tmpf#115 as double)) AS stddev_temp#433]\n      +- Filter isnotnull(valid#114-T900000ms)\n         +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) as double) = (cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 604800000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) as double) = (cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 604800000000) + 0) + 604800000000), LongType, TimestampType)) AS window#442-T900000ms, event_key#110, event_topic#111, event_timestamp#112, station#113, valid#114-T900000ms, tmpf#115, dwpf#116, relh#117, feel#118, drct#119, sped#120, alti#121, mslp#122, p01m#123, vsby#124, skyc1#125, skyl1#126, wxcodes#127, ice_acceretion_1hr#128]\n            +- EventTimeWatermark valid#114: timestamp, 15 minutes\n               +- Filter (((hour(valid#114, Some(Etc/UTC)) >= 19) AND (hour(valid#114, Some(Etc/UTC)) <= 23)) OR ((hour(valid#114, Some(Etc/UTC)) >= 0) AND (hour(valid#114, Some(Etc/UTC)) < 8)))\n                  +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n                     +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                        +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                           +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_day_night \u001b[38;5;241m=\u001b[39m \u001b[43mdf_day_night\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/streaming.py:1211\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;;\nUnion\n:- Aggregate [station#113, window#377-T604800000ms], [station#113, window#377-T604800000ms AS window#337-T604800000ms, avg(cast(tmpf#115 as double)) AS avg_temp#358, stddev_samp(cast(tmpf#115 as double)) AS stddev_temp#368]\n:  +- Filter isnotnull(valid#114-T604800000ms)\n:     +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) as double) = (cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 604800000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) as double) = (cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#114-T604800000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 604800000000) + 0) + 604800000000), LongType, TimestampType)) AS window#377-T604800000ms, event_key#110, event_topic#111, event_timestamp#112, station#113, valid#114-T604800000ms, tmpf#115, dwpf#116, relh#117, feel#118, drct#119, sped#120, alti#121, mslp#122, p01m#123, vsby#124, skyc1#125, skyl1#126, wxcodes#127, ice_acceretion_1hr#128]\n:        +- EventTimeWatermark valid#114: timestamp, 7 days\n:           +- Filter ((hour(valid#114, Some(Etc/UTC)) >= 8) AND (hour(valid#114, Some(Etc/UTC)) < 19))\n:              +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n:                 +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n:                    +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n:                       +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n:                          +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n+- Project [station#113 AS station#1033, window#402-T900000ms AS window#1034-T900000ms, avg_temp#423 AS avg_temp#1035, stddev_temp#433 AS stddev_temp#1036]\n   +- Aggregate [station#113, window#442-T900000ms], [station#113, window#442-T900000ms AS window#402-T900000ms, avg(cast(tmpf#115 as double)) AS avg_temp#423, stddev_samp(cast(tmpf#115 as double)) AS stddev_temp#433]\n      +- Filter isnotnull(valid#114-T900000ms)\n         +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) as double) = (cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 604800000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) as double) = (cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#114-T900000ms, TimestampType, LongType) - 0) as double) / cast(604800000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 604800000000) + 0) + 604800000000), LongType, TimestampType)) AS window#442-T900000ms, event_key#110, event_topic#111, event_timestamp#112, station#113, valid#114-T900000ms, tmpf#115, dwpf#116, relh#117, feel#118, drct#119, sped#120, alti#121, mslp#122, p01m#123, vsby#124, skyc1#125, skyl1#126, wxcodes#127, ice_acceretion_1hr#128]\n            +- EventTimeWatermark valid#114: timestamp, 15 minutes\n               +- Filter (((hour(valid#114, Some(Etc/UTC)) >= 19) AND (hour(valid#114, Some(Etc/UTC)) <= 23)) OR ((hour(valid#114, Some(Etc/UTC)) >= 0) AND (hour(valid#114, Some(Etc/UTC)) < 8)))\n                  +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n                     +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                        +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                           +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n"
     ]
    }
   ],
   "source": [
    "query_day_night = df_day_night\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_day_night.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-hour Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "df_windowed_2hrs = (df_formatted\n",
    "    .withWatermark(\"valid\", \"30 minutes\")\n",
    "    .groupBy(\"station\", window(\"valid\", \"2 hours\"))\\\n",
    "    .agg(collect_list(\"tmpf\").alias(\"temp_list\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:39:09 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2e9516df-1dc8-452c-9801-29b3a3681323. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "query_5=df_windowed_2hrs\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_exploded = df_windowed_2hrs.select(\"Window\", \"station\", explode(\"temp_list\").alias(\"temperature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:39:11 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2399f2d2-9628-4ca6-9110-90de9f831185. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "query_6= (df_exploded\n",
    "            .writeStream\n",
    "            .outputMode(\"complete\")\n",
    "            .format(\"console\")\n",
    "            .option(\"truncate\",'false')\n",
    "            .trigger(processingTime=\"10 seconds\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_6.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_day_night.join(\n",
    "    df_exploded,\n",
    "    on=(df_day_night[\"station\"] == df_exploded[\"station\"]) \n",
    "    & (df_day_night[\"valid\"] >= df_exploded[\"Window.start\"]) \n",
    "    & (df_day_night[\"valid\"] <= df_exploded[\"Window.end\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:39:42 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ee1b765c-c311-4fdc-a82c-afed4072e95a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/12 06:39:42 WARN UnsupportedOperationChecker: Detected pattern of possible 'correctness' issue due to global watermark. The query contains stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are \"late rows\" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details.;\n",
      "Join Inner, (((station#113 = station#701) AND (valid#114 >= Window#524-T1800000ms.start)) AND (valid#114 <= Window#524-T1800000ms.end))\n",
      ":- Union\n",
      ":  :- Filter ((hour(valid#114, Some(Etc/UTC)) >= 8) AND (hour(valid#114, Some(Etc/UTC)) < 19))\n",
      ":  :  +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n",
      ":  :     +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      ":  :        +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      ":  :           +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      ":  :              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
      ":  +- Project [event_key#110 AS event_key#467, event_topic#111 AS event_topic#468, event_timestamp#112 AS event_timestamp#469, station#113 AS station#470, valid#114 AS valid#471, tmpf#115 AS tmpf#472, dwpf#116 AS dwpf#473, relh#117 AS relh#474, feel#118 AS feel#475, drct#119 AS drct#476, sped#120 AS sped#477, alti#121 AS alti#478, mslp#122 AS mslp#479, p01m#123 AS p01m#480, vsby#124 AS vsby#481, skyc1#125 AS skyc1#482, skyl1#126 AS skyl1#483, wxcodes#127 AS wxcodes#484, ice_acceretion_1hr#128 AS ice_acceretion_1hr#485]\n",
      ":     +- Filter (((hour(valid#114, Some(Etc/UTC)) >= 19) AND (hour(valid#114, Some(Etc/UTC)) <= 23)) OR ((hour(valid#114, Some(Etc/UTC)) >= 0) AND (hour(valid#114, Some(Etc/UTC)) < 8)))\n",
      ":        +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n",
      ":           +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      ":              +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      ":                 +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      ":                    +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
      "+- Project [Window#524-T1800000ms, station#701, temperature#556]\n",
      "   +- Generate explode(temp_list#545), false, [temperature#556]\n",
      "      +- Aggregate [station#701, window#546-T1800000ms], [station#701, window#546-T1800000ms AS window#524-T1800000ms, collect_list(tmpf#703, 0, 0) AS temp_list#545]\n",
      "         +- Filter isnotnull(valid#702-T1800000ms)\n",
      "            +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#702-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) as double) = (cast((precisetimestampconversion(valid#702-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#702-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#702-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 7200000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#702-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) as double) = (cast((precisetimestampconversion(valid#702-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#702-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#702-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 7200000000) + 0) + 7200000000), LongType, TimestampType)) AS window#546-T1800000ms, event_key#698, event_topic#699, event_timestamp#700, station#701, valid#702-T1800000ms, tmpf#703, dwpf#704, relh#705, feel#706, drct#707, sped#708, alti#709, mslp#710, p01m#711, vsby#712, skyc1#713, skyl1#714, wxcodes#715, ice_acceretion_1hr#716]\n",
      "               +- EventTimeWatermark valid#702: timestamp, 30 minutes\n",
      "                  +- Project [key#21 AS event_key#698, topic#9 AS event_topic#699, timestamp#12 AS event_timestamp#700, value#102.station AS station#701, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#702, value#102.tmpf AS tmpf#703, value#102.dwpf AS dwpf#704, value#102.relh AS relh#705, value#102.feel AS feel#706, value#102.drct AS drct#707, value#102.sped AS sped#708, value#102.alti AS alti#709, value#102.mslp AS mslp#710, value#102.p01m AS p01m#711, value#102.vsby AS vsby#712, value#102.skyc1 AS skyc1#713, value#102.skyl1 AS skyl1#714, value#102.wxcodes AS wxcodes#715, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#716]\n",
      "                     +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      "                        +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      "                           +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      "                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----------+---------------+-------+-----+----+----+----+----+----+----+----+----+----+----+-----+-----+-------+------------------+------+-------+-----------+\n",
      "|event_key|event_topic|event_timestamp|station|valid|tmpf|dwpf|relh|feel|drct|sped|alti|mslp|p01m|vsby|skyc1|skyl1|wxcodes|ice_acceretion_1hr|Window|station|temperature|\n",
      "+---------+-----------+---------------+-------+-----+----+----+----+----+----+----+----+----+----+----+-----+-----+-------+------------------+------+-------+-----------+\n",
      "+---------+-----------+---------------+-------+-----+----+----+----+----+----+----+----+----+----+----+-----+-----+-------+------------------+------+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:40:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 57819 milliseconds\n",
      "[Stage 7:======================================================>(198 + 1) / 200]\r"
     ]
    }
   ],
   "source": [
    "query_7= (joined_df\n",
    "            .writeStream\n",
    "            .outputMode(\"append\")\n",
    "            .format(\"console\")\n",
    "            .option(\"truncate\",'false')\n",
    "            .trigger(processingTime=\"10 seconds\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 06:41:06 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@2379cd94 is aborting.\n",
      "23/04/12 06:41:06 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@2379cd94 aborted.\n",
      "23/04/12 06:41:07 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 606, 172.18.0.7, executor 1): TaskKilled (Stage cancelled)\n",
      "23/04/12 06:41:07 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 605, 172.18.0.6, executor 0): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "query_7.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`avg_temp`' given input columns: [Window, alti, drct, dwpf, event_key, event_timestamp, event_topic, feel, ice_acceretion_1hr, mslp, p01m, relh, skyc1, skyl1, sped, station, station, temperature, tmpf, valid, vsby, wxcodes]; line 1 pos 8;\n'Project [event_key#110, event_topic#111, event_timestamp#112, station#113, valid#114, tmpf#115, dwpf#116, relh#117, feel#118, drct#119, sped#120, alti#121, mslp#122, p01m#123, vsby#124, skyc1#125, skyl1#126, wxcodes#127, ice_acceretion_1hr#128, Window#524-T1800000ms, station#569, temperature#556, ((tmpf#115 - 'avg_temp) / 'stddev_temp) AS z_score#695]\n+- Join Inner, (((station#113 = station#569) AND (valid#114 >= Window#524-T1800000ms.start)) AND (valid#114 <= Window#524-T1800000ms.end))\n   :- Union\n   :  :- Filter ((hour(valid#114, Some(Etc/UTC)) >= 8) AND (hour(valid#114, Some(Etc/UTC)) < 19))\n   :  :  +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n   :  :     +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :  :        +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :  :           +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :  :              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n   :  +- Project [event_key#110 AS event_key#467, event_topic#111 AS event_topic#468, event_timestamp#112 AS event_timestamp#469, station#113 AS station#470, valid#114 AS valid#471, tmpf#115 AS tmpf#472, dwpf#116 AS dwpf#473, relh#117 AS relh#474, feel#118 AS feel#475, drct#119 AS drct#476, sped#120 AS sped#477, alti#121 AS alti#478, mslp#122 AS mslp#479, p01m#123 AS p01m#480, vsby#124 AS vsby#481, skyc1#125 AS skyc1#482, skyl1#126 AS skyl1#483, wxcodes#127 AS wxcodes#484, ice_acceretion_1hr#128 AS ice_acceretion_1hr#485]\n   :     +- Filter (((hour(valid#114, Some(Etc/UTC)) >= 19) AND (hour(valid#114, Some(Etc/UTC)) <= 23)) OR ((hour(valid#114, Some(Etc/UTC)) >= 0) AND (hour(valid#114, Some(Etc/UTC)) < 8)))\n   :        +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n   :           +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :              +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :                 +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :                    +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n   +- Project [Window#524-T1800000ms, station#569, temperature#556]\n      +- Generate explode(temp_list#545), false, [temperature#556]\n         +- Aggregate [station#569, window#546-T1800000ms], [station#569, window#546-T1800000ms AS window#524-T1800000ms, collect_list(tmpf#571, 0, 0) AS temp_list#545]\n            +- Filter isnotnull(valid#570-T1800000ms)\n               +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) as double) = (cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 7200000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) as double) = (cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 7200000000) + 0) + 7200000000), LongType, TimestampType)) AS window#546-T1800000ms, event_key#566, event_topic#567, event_timestamp#568, station#569, valid#570-T1800000ms, tmpf#571, dwpf#572, relh#573, feel#574, drct#575, sped#576, alti#577, mslp#578, p01m#579, vsby#580, skyc1#581, skyl1#582, wxcodes#583, ice_acceretion_1hr#584]\n                  +- EventTimeWatermark valid#570: timestamp, 30 minutes\n                     +- Project [key#21 AS event_key#566, topic#9 AS event_topic#567, timestamp#12 AS event_timestamp#568, value#102.station AS station#569, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#570, value#102.tmpf AS tmpf#571, value#102.dwpf AS dwpf#572, value#102.relh AS relh#573, value#102.feel AS feel#574, value#102.drct AS drct#575, value#102.sped AS sped#576, value#102.alti AS alti#577, value#102.mslp AS mslp#578, value#102.p01m AS p01m#579, value#102.vsby AS vsby#580, value#102.skyc1 AS skyc1#581, value#102.skyl1 AS skyl1#582, value#102.wxcodes AS wxcodes#583, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#584]\n                        +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                           +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                              +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                                 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_with_zscore \u001b[38;5;241m=\u001b[39m \u001b[43mjoined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(tmpf - avg_temp) / stddev_temp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:2096\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2093\u001b[0m \n\u001b[1;32m   2094\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`avg_temp`' given input columns: [Window, alti, drct, dwpf, event_key, event_timestamp, event_topic, feel, ice_acceretion_1hr, mslp, p01m, relh, skyc1, skyl1, sped, station, station, temperature, tmpf, valid, vsby, wxcodes]; line 1 pos 8;\n'Project [event_key#110, event_topic#111, event_timestamp#112, station#113, valid#114, tmpf#115, dwpf#116, relh#117, feel#118, drct#119, sped#120, alti#121, mslp#122, p01m#123, vsby#124, skyc1#125, skyl1#126, wxcodes#127, ice_acceretion_1hr#128, Window#524-T1800000ms, station#569, temperature#556, ((tmpf#115 - 'avg_temp) / 'stddev_temp) AS z_score#695]\n+- Join Inner, (((station#113 = station#569) AND (valid#114 >= Window#524-T1800000ms.start)) AND (valid#114 <= Window#524-T1800000ms.end))\n   :- Union\n   :  :- Filter ((hour(valid#114, Some(Etc/UTC)) >= 8) AND (hour(valid#114, Some(Etc/UTC)) < 19))\n   :  :  +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n   :  :     +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :  :        +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :  :           +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :  :              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n   :  +- Project [event_key#110 AS event_key#467, event_topic#111 AS event_topic#468, event_timestamp#112 AS event_timestamp#469, station#113 AS station#470, valid#114 AS valid#471, tmpf#115 AS tmpf#472, dwpf#116 AS dwpf#473, relh#117 AS relh#474, feel#118 AS feel#475, drct#119 AS drct#476, sped#120 AS sped#477, alti#121 AS alti#478, mslp#122 AS mslp#479, p01m#123 AS p01m#480, vsby#124 AS vsby#481, skyc1#125 AS skyc1#482, skyl1#126 AS skyl1#483, wxcodes#127 AS wxcodes#484, ice_acceretion_1hr#128 AS ice_acceretion_1hr#485]\n   :     +- Filter (((hour(valid#114, Some(Etc/UTC)) >= 19) AND (hour(valid#114, Some(Etc/UTC)) <= 23)) OR ((hour(valid#114, Some(Etc/UTC)) >= 0) AND (hour(valid#114, Some(Etc/UTC)) < 8)))\n   :        +- Project [key#21 AS event_key#110, topic#9 AS event_topic#111, timestamp#12 AS event_timestamp#112, value#102.station AS station#113, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#114, value#102.tmpf AS tmpf#115, value#102.dwpf AS dwpf#116, value#102.relh AS relh#117, value#102.feel AS feel#118, value#102.drct AS drct#119, value#102.sped AS sped#120, value#102.alti AS alti#121, value#102.mslp AS mslp#122, value#102.p01m AS p01m#123, value#102.vsby AS vsby#124, value#102.skyc1 AS skyc1#125, value#102.skyl1 AS skyl1#126, value#102.wxcodes AS wxcodes#127, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#128]\n   :           +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :              +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :                 +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n   :                    +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n   +- Project [Window#524-T1800000ms, station#569, temperature#556]\n      +- Generate explode(temp_list#545), false, [temperature#556]\n         +- Aggregate [station#569, window#546-T1800000ms], [station#569, window#546-T1800000ms AS window#524-T1800000ms, collect_list(tmpf#571, 0, 0) AS temp_list#545]\n            +- Filter isnotnull(valid#570-T1800000ms)\n               +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) as double) = (cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 7200000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) as double) = (cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) THEN (CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(valid#570-T1800000ms, TimestampType, LongType) - 0) as double) / cast(7200000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 7200000000) + 0) + 7200000000), LongType, TimestampType)) AS window#546-T1800000ms, event_key#566, event_topic#567, event_timestamp#568, station#569, valid#570-T1800000ms, tmpf#571, dwpf#572, relh#573, feel#574, drct#575, sped#576, alti#577, mslp#578, p01m#579, vsby#580, skyc1#581, skyl1#582, wxcodes#583, ice_acceretion_1hr#584]\n                  +- EventTimeWatermark valid#570: timestamp, 30 minutes\n                     +- Project [key#21 AS event_key#566, topic#9 AS event_topic#567, timestamp#12 AS event_timestamp#568, value#102.station AS station#569, to_timestamp('value.valid, Some(yyyy-MM-dd HH:mm)) AS valid#570, value#102.tmpf AS tmpf#571, value#102.dwpf AS dwpf#572, value#102.relh AS relh#573, value#102.feel AS feel#574, value#102.drct AS drct#575, value#102.sped AS sped#576, value#102.alti AS alti#577, value#102.mslp AS mslp#578, value#102.p01m AS p01m#579, value#102.vsby AS vsby#580, value#102.skyc1 AS skyc1#581, value#102.skyl1 AS skyl1#582, value#102.wxcodes AS wxcodes#583, value#102.ice_acceretion_1hr AS ice_acceretion_1hr#584]\n                        +- Project [key#21, from_json(StructField(station,StringType,true), StructField(valid,StringType,true), StructField(tmpf,StringType,true), StructField(dwpf,StringType,true), StructField(relh,StringType,true), StructField(feel,StringType,true), StructField(drct,StringType,true), StructField(sped,StringType,true), StructField(alti,StringType,true), StructField(mslp,StringType,true), StructField(p01m,StringType,true), StructField(vsby,StringType,true), StructField(skyc1,StringType,true), StructField(skyl1,StringType,true), StructField(wxcodes,StringType,true), StructField(ice_acceretion_1hr,StringType,true), value#29, Some(Etc/UTC)) AS value#102, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                           +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                              +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                                 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2f1d20c3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@179a7088, org.apache.spark.sql.util.CaseInsensitiveStringMap@e20c70b1, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@68afa7b5,kafka,List(),None,List(),None,Map(subscribe -> topic_test1, kafka.bootstrap.servers -> kafka:9093),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n"
     ]
    }
   ],
   "source": [
    "df_with_zscore = joined_df.withColumn('z_score', expr('(tmpf - avg_temp) / stddev_temp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalous_requests = (df_with_zscore.filter(~isnan(col(\"z_score\")))\n",
    "                      .filter(\"z_score > 2 or z_score < -2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_query= (df_anomalous_requests\n",
    "            .writeStream\n",
    "            .outputMode(\"complete\")\n",
    "            .format(\"console\")\n",
    "            .option(\"truncate\",'false')\n",
    "            .trigger(processingTime=\"10 seconds\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
